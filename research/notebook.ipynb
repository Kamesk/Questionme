{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import numpy as np\n",
    "import re\n",
    "from langchain.schema import Document\n",
    "from langchain.llms import CTransformers\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing all documents from the folder - artifacts and cleaning - /n removed, - replaced. \n",
    "path_to_pdfs = (r\"D:\\LLM\\projects\\Questionme\\artifacts\")\n",
    "loader = PyPDFDirectoryLoader(path_to_pdfs)\n",
    "pdf_documents = loader.load()\n",
    "\n",
    "def cleanup(document):\n",
    "    document = document.replace('\\n', ' ')\n",
    "    document = document.replace('-', '')\n",
    "    document = ' '.join(document.split())\n",
    "    document = re.sub(r'\\n+', ' ', document)\n",
    "    return document\n",
    "\n",
    "cleaned_documents = [cleanup(doc.page_content) for doc in pdf_documents]\n",
    "cleaned_document_objects = [Document(page_content=cleaned_text, metadata=doc.metadata) \n",
    "                             for cleaned_text, doc in zip(cleaned_documents, pdf_documents)]\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "final_documents=text_splitter.split_documents(cleaned_document_objects)\n",
    "texts = [doc.page_content for doc in final_documents]\n",
    "text = texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(\n",
    "    model=r\"D:\\LLM\\models\\llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "    model_type=\"llama\",\n",
    "    config={'max_new_tokens': 300, 'temperature': 0.03,'context_length': 1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jarviz_92\\anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "huggingface_embeddings=HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",      #sentence-transformers/all-MiniLM-l6-v2\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06778146  0.01100297  0.02574205 ...  0.00751806  0.06441373\n",
      "  -0.03051739]\n",
      " [-0.0693738  -0.02106547 -0.03994767 ... -0.00298001  0.05587736\n",
      "   0.04344444]\n",
      " [-0.03051266  0.08318012 -0.02475407 ... -0.06984834  0.04987878\n",
      "   0.01845735]\n",
      " ...\n",
      " [ 0.05832506  0.07038648  0.02622284 ...  0.07168836 -0.01516775\n",
      "  -0.02078729]\n",
      " [ 0.08588448  0.01911633  0.0015234  ...  0.04203937  0.02636152\n",
      "  -0.02784703]\n",
      " [-0.01472853 -0.00520878 -0.00508864 ...  0.07673403 -0.00523054\n",
      "  -0.04715177]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embeddings = [huggingface_embeddings.embed_query(text) for text in texts]\n",
    "embeddings_array = np.array(embeddings)\n",
    "print(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(final_documents, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Not sure. I'm just an AI and do not have access to the most up-to-date information on UK parking rules, penalties, and fixed penalty details. It is important to consult official sources such as the Rural Payments Agency or the GOV.UK website for the most accurate and reliable information. Additionally, if you have any questions or concerns about enforcement or penalties procedures, you may contact the Meat Technical Schemes team at the Rural Payments Agency directly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "You are an expert in UK parking rules, penalties, and fixed penalty details. Answer the question precisely. If you're not 100 percent sure of the answer, respond with 'not sure' and request the user to check updated and correct information or reach a trustworthy solicitor.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=['context', 'question'])\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={'prompt': qa_prompt}\n",
    ")\n",
    "\n",
    "user_input = \"Who sets the level of FPN's\"\n",
    "\n",
    "# Retrieve context from the retriever using the correct method\n",
    "retrieved_docs = retriever.get_relevant_documents(user_input)  # Correct method usage\n",
    "\n",
    "# Define a function to truncate text to a maximum number of tokens\n",
    "def truncate_to_token_limit(text, max_tokens):\n",
    "    tokens = text.split()  # Simple tokenization\n",
    "    return ' '.join(tokens[:max_tokens]) if len(tokens) > max_tokens else text\n",
    "\n",
    "# Set a maximum token limit for truncating documents\n",
    "max_tokens_per_doc = 100  # Example limit for each document\n",
    "\n",
    "# Truncate each document's content to the specified token limit\n",
    "truncated_context = [truncate_to_token_limit(doc.page_content, max_tokens_per_doc) for doc in retrieved_docs]\n",
    "\n",
    "# Calculate the total token count for all inputs\n",
    "total_tokens = len(user_input.split()) + sum(len(doc.split()) for doc in truncated_context)\n",
    "\n",
    "# Define the maximum total tokens allowed\n",
    "max_total_tokens = 512\n",
    "\n",
    "# Adjust context to ensure total tokens do not exceed the maximum\n",
    "if total_tokens > max_total_tokens:\n",
    "    # Truncate further if needed to fit within the total token limit\n",
    "    truncated_context = truncated_context[:max_total_tokens - len(user_input.split())]\n",
    "\n",
    "# Ensure the final context is within the allowed token limit\n",
    "final_context = ' '.join(truncated_context)\n",
    "\n",
    "# Check again to avoid exceeding the token limit\n",
    "if len(final_context.split()) + len(user_input.split()) > max_total_tokens:\n",
    "    print(\"Final context exceeds maximum token limit. Adjusting to fit.\")\n",
    "    final_context = truncate_to_token_limit(final_context, max_total_tokens - len(user_input.split()))\n",
    "\n",
    "# Call the chain with the query and the truncated context\n",
    "try:\n",
    "    result = chain({'query': user_input, 'context': final_context})  # Correct input keys\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
